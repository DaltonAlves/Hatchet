{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p00yepXXVUH1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating list of urls and scrapping the 'archives' landing page of the Hatchet to fill list with URLs of each volume summary page.\n",
        "URL = 'https://www.gwhatchet.com/archives/'\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.text)\n",
        "\n",
        "url_list = []\n",
        "\n",
        "body_class = soup.select_one('div.primary-container') #select div with just the volume headings and links\n",
        "for link in body_class.findAll('a')[0:5]: #limiting to the first few urls for the purposes of testing. remove this range if desired to scrap ALL links\n",
        "    url_list.append(link.get('href'))\n",
        "\n",
        "complete_url_list = []\n",
        "\n",
        "for urls in url_list: ##this for loop will populate a new list with page numbers 1-7 for each url in the url list. probably a better way to do this, but it works for now\n",
        "  count = 1\n",
        "  while count < 8:\n",
        "    complete_url_list.append(urls + 'page/' +str(count))\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "UJRuTc_TVa2O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dataframe to hold info\n",
        "data = pd.DataFrame(columns=['headline', 'author', 'category', 'content', 'full_url'])"
      ],
      "metadata": {
        "id": "ytNMABOZVzuF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for link in range(len(complete_url_list)): ##loop through URLs to create soup for each url in complete_url_list\n",
        "  result = requests.get(complete_url_list[link])\n",
        "  html = result.content\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "  for post in soup.find_all('article', attrs ={'class' : 'post'}):  ##use each soup to extract headline, author, category, content, and url info\n",
        "    try:\n",
        "      headline = post.find('h2', attrs ={'class' : 'post-title'}).text\n",
        "    except:\n",
        "      print('No Headline found')\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      author = post.find('span', attrs = {'class' : 'byline-author'}).text.strip('By ') #stripping By and whitespace #####this line will error out because some posts lack author\n",
        "    except: \n",
        "      print('No Author found')\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      category = post.find('div', attrs={'class' : 'category-badge'}).text.strip() \n",
        "    except:\n",
        "      print('No Category found')\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      content = post.find('div', attrs={'class' : 'post-content'}).text.strip()\n",
        "    except:\n",
        "      print('No Content found')\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "     full_url = post.find('h2').find('a').get('href')\n",
        "    except:\n",
        "      print('No full_url found')\n",
        "      pass\n",
        "\n",
        "    data = data.append({'headline': headline, 'author': author, 'category': category, 'content': content, 'full_url': full_url}, ignore_index=True) #add extracted info to dataframe\n",
        "\n",
        "    sleep(1) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cR24jzmWrjO",
        "outputId": "e916a0dd-9d7f-4831-da51-163a54d63438"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Author found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv (r'hatchet_dataframe.csv', index = False, header = True)"
      ],
      "metadata": {
        "id": "hpeOpc5Gci-q"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}